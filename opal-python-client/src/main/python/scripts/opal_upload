#!/usr/bin/python

from __future__ import print_function, division

import sys, os, errno
import re
import json
import time
import csv
import shutil
from os import path, listdir
from cached_property import cached_property
from opal_tools_lib import *

verbose = '-v' in sys.argv
quiet = '-q' in sys.argv


## Parse config file ##

CONFIGFILE = '~/.opal/fileupload.conf'

def setup():
    global file_type, done, src_folder, done_folder, opal_folder

    with setup_loader(CONFIGFILE) as config:

        file_type = config.get('main', 'file_type')
        done = config.get('main', 'done')
        src_folder = expanduser(config.get('main', 'src_folder'))
        done_folder = expanduser(config.get('main', 'done_folder')) if done == 'move' else None
        opal_folder = config.get('main', 'opal_folder')

        if file_type != 'csv':
            raise KnownError(
                'Configuration file error: file_type = {0}. File types other than "csv" are not currently '
                'supported'.format(file_type))
        if done not in ('move', 'delete', 'none'):
            raise KnownError(
                'Configureation file error: done = {0}, should be one of "move", "delete" or "none"'.format(done))

        if done == 'move':
            if not done_folder.endswith('/'):
                done_folder += '/'
            if not path.isdir(done_folder):
                raise KnownError("done_folder '{0}' does not exist".format(done_folder))


class FileNamePatternException (KnownError):
    pass

class OpalFile:

    reindex_tables = []
    operations = ['update', 'delete', 'replace']
    binary_files = set()

    # Retain the order in which the files were processed, but only reindex each project once.
    @classmethod
    def reindex_all(cls):
        logging.info('starting reindex for tables {0}'.format(', '.join(cls.reindex_tables)))
        run_rest_command_params('/indexes', ('table='+u(t) for t in cls.reindex_tables), 'PUT')
        del cls.reindex_tables[:]

    @classmethod
    def cleanup_binaries(cls):
        if not cls.binary_files:
            return
        logging.info("Cleaning up binary files")
        cleanup(cls.binary_files)
        cls.binary_files.clear()


    def __init__(self, filename):
        self.name = filename
        # filename should be in the format <timestamp>.<project_name>.<table_name>.<operation>.csv
        # example: 20140503140523.mdsantwerp.lifelines.update.csv
        match = re.match(r'^([^\.]+)\.([^\.]+)\.([^\.]+)\.([^\.]+)(.*)\.csv$', filename)
        if match:
            self.timestamp, self.project, self.table, self.operation = match.groups()[0:4]
        if not match or self.operation not in self.operations:
            raise FileNamePatternException(
                'File name {0} does not match expected format: '
                '<timestamp>.<project_name>.<table_name>.<operation>.csv\n'
                'Valid operations: update, delete, replace'.format(filename))
        self.path = path.join(src_folder, filename)

    @property
    def tableref(self):
        return self.project + '.' + self.table

    @cached_property
    def entity_type(self):
        logging.log(LOGINFO, 'loading entity type for table {0}'.format(self.tableref))
        res = json.loads(run_rest_command(
            '/datasource/{0}/table/{1}'.format(u(self.project), u(self.table))))
        return res['entityType']

    @cached_property
    def variables(self):
        logging.log(LOGINFO, 'loading variables for table {0}'.format(self.tableref))
        return json.loads(run_rest_command(
            '/datasource/{0}/table/{1}/variables'.format(u(self.project), u(self.table))))


    # processes this file
    def process(self):
        logging.info('processing '+self.name)
        start_time = time.time()

        if not self.operation in self.operations:
            raise Exception("Operation '{0}' not implemented".format(self.operation))

        getattr(self, self.operation)()

        cleanup([self.name], remove_from_opal= self.operation != 'delete')

        logging.info('finished processing {0} in {1:.3f} seconds'.format(self.name, time.time() - start_time))

    def update(self):
        self.do_upload()
        try:
            self.do_import()
        finally:
            self.schedule_reindex()

    def replace(self):
        self.do_upload()
        self.do_truncate()
        self.schedule_reindex()
        self.do_import()

    def delete(self):
        # Delete ids are sent as url parameters. Sending a too large url results in an error 413 FULL HEAD,
        # so we need to split large deletions up over several requests.
        with open(self.path) as idfile:
            size = filesize(idfile)
            url = '/datasource/{0}/table/{1}/valueSets'.format(u(self.project), u(self.table))

            if quiet:
                callback = self.schedule_reindex
            elif size:
                def callback():
                    sys.stderr.write('\rDeleting, {0:.0%} completed...'.format(idfile.tell() / size))
                    self.schedule_reindex()
            else:
                def callback():
                    sys.stderr.write('.')
                    self.schedule_reindex()

            if not quiet and not size: sys.stderr.write('Deleting.')

            params = ('id='+u(id) for id in self.delete_ids(idfile))
            run_rest_command_params(url, params, method='DELETE', progresscallback=callback)

            if not quiet: sys.stderr.write('done\n')

    def delete_ids(self, idfile):
        reader = csv.reader(idfile)
        # skip header
        reader.next()
        for l in reader:
            yield l[0]

    def do_truncate(self):
        logging.info('Truncating table {0}'.format(self.tableref))
        run_rest_command(
            '/datasource/{0}/table/{1}/valueSets'.format(u(self.project), u(self.table)),
            method='DELETE')

    def do_upload(self):
        binary_vars = [v['name'] for v in self.variables if v['valueType'] == 'binary']
        if binary_vars:
            self.do_upload_blobs(binary_vars)

        upload_file(self.path, opal_folder)

    def do_upload_blobs(self, binary_vars):
        logging.info("Uploading binary blobs for {0}".format(self.name))
        with open(self.path) as csvfile:
            reader = csv.reader(csvfile)
            header = reader.next()
            binary_columns = [header.index(var) for var in binary_vars if var in header]
            if not binary_columns:
                return
            max_bin = max(binary_columns)
            for line in reader:
                if len(line) <= max_bin:
                    raise KnownError("No value found for variable '{0}' while trying to process {1}".format(
                        header[max_bin], self.name))
                for idx in binary_columns:
                    blob = line[idx]
                    # Don't re-upload a file if it was already uploaded
                    if blob in self.binary_files:
                        continue
                    src = path.join(src_folder, blob)
                    try:
                        upload_file(src, opal_folder)
                        self.binary_files.add(blob)
                    except subprocess.CalledProcessError as e:
                        if e.returncode == 2 and r"couldn\'t open file" in e.output:
                            raise KnownError("File not found: '{0}', specified as binary value for variable '{1}'"
                                             " for id '{2}' in {3}".format(
                                src, header[idx], line[0], self.name))
                        raise

    def do_import(self):
        # opal import-csv -o <opal_base_url> -u <username> -p <password> -d <project> -i
        #      -pa <opalfs_self_path> -ty <entity_type> -t <table>
        import_cmd = ['opal', 'import-csv'] + auth_params + ['-d', self.project, '-i', '-ncv',
                      '-pa', opal_folder+'/'+self.name, '-ty', self.entity_type, '-t', self.table]
        result = json.loads(run_command(import_cmd))
        job_id = result['id']
        status = result['status']
        percent = result.get('progress', {}).get('percent', 0)
        logging.log(LOGINFO, "Import job ID: "+str(job_id))
        if not quiet: sys.stderr.write('Importing data (job id {0})\n'.format(job_id))
        while status == 'IN_PROGRESS':
            if not quiet: sys.stderr.write('\rprogress {0}%'.format(percent))
            logging.log(LOGINFO, "Progress on job {0}: {1}%".format(job_id, percent))
            time.sleep(1)
            result = json.loads(run_rest_command(
                '/project/{0}/command/{1}'.format(u(self.project), u(job_id))))
            status = result['status']
            percent = result.get('progress', {}).get('percent', 0)
        if not quiet: sys.stderr.write('...done\n')
        if status == 'FAILED':
            err = []
            for msg in result['messages']:
                match = re.match(r'Variables do not exist in (.*) and creating new variables is disabled: (.*)', msg['msg'])
                if match:
                    err.append("variable(s) {0} do not exist in table {1}".format(match.groups()[1], match.groups()[0]))
            if err:
                err.append("make sure the first row of the .csv file is a header row.")
                raise KnownError(', '.join(err))

            raise Exception('Import job failed: {0}\nJob messages:\n{1}'.format(
                self.name, '\n'.join('* '+m['msg'].strip() for m in result['messages'])))

    def schedule_reindex(self):
        if self.project not in self.reindex_tables:
            self.reindex_tables.append(self.tableref)


def cleanup(files, remove_from_opal=True):
    for file in files:
        # remove file from opal file system
        if remove_from_opal:
            delete_opal_file(file)
        globals()['cleanup_'+done](file)

def cleanup_move(file):
    # Remove local target if it exists
    src_path = path.join(src_folder, file)
    try:
        destpath = path.join(done_folder, file)
        if path.exists(destpath):
            logging.warn('Destination {0} exists, attempting to overwrite'.format(destpath))
            os.remove(destpath)
    except OSError as e:
        logging.error(e)
        logging.log(LOGINFO, e, exc_info=True)

    # Move local file to done folder
    logging.info('moving {0} to {1}'.format(src_path, done_folder))
    try:
        shutil.move(src_path, done_folder)
    except OSError as e:
        raise KnownError("Cannot move file to done_folder: "+str(e))

def cleanup_delete(file):
    logging.info('deleting {0}'.format(file))
    os.remove(file)

def cleanup_none(file):
    pass

def delete_opal_file(file):
    "Remove file from opal filesystem /data_import"
    # opal file -o <url> -u administrator -p password --delete /data_import/sample.csv
    remove_cmd = ['opal', 'file'] + auth_params + ['--delete', opal_folder + '/' + file, '-f']
    run_command(remove_cmd)


def filesize(file):
    "Get the file size of an open file. The file must be seekable, otherwise returns None."
    try:
        curpos = file.tell()
        file.seek(0, os.SEEK_END)
        size = file.tell()
        file.seek(curpos)
    except IOError as e:
        if e.errno != errno.ESPIPE:
            raise
        size = None
    return size


def upload_file(src, opal_dest):
    "Upload a file specified by src to opal in the opal_dest folder"
    #opal file -o <url> -u administrator -p password -up sample.csv /data_import
    #opal file -o http://localhost:8080 -sc server.crt -sk server.key -up sample.csv /data_import
    logging.info('Uploading '+src)
    upload_cmd = ['opal', 'file'] + auth_params + ['-up', src, opal_dest]
    try:
        run_command(upload_cmd)
    except subprocess.CalledProcessError as e:
        if '404 Not Found' in e.output:
            raise KnownError("Folder '{0}' does not exist on the server".format(opal_dest))
        raise


# returns a list of ImportFile, ordered by date (oldest first)
def get_files():
    list = []
    try:
        files = get_all_files()
    except OSError as e:
        raise KnownError("Can not access src_folder: " + str(e))
    for f in files:
        fullname = path.join(src_folder, f)
        if isfile(fullname):
            try:
                list.append(OpalFile(f))
            except FileNamePatternException:
                logging.log(logging.DEBUG, "File {0} is not a table operation file, skipping".format(f))

    list.sort(key=lambda f: f.name.split('_', 1)[0])
    return list

all_files = None
def get_all_files():
    global all_files
    if all_files == None:
        all_files = set(listdir(src_folder))
    return all_files

def main():
    retcode = 0
    try:
        files = get_files()
        if not files:
            logging.warn('No data files found, nothing to do')
            sys.exit(0)
        for file in files:
            file.process()
    except Exception as e:
        handle_exception(e)
        retcode |= 1

    # We try to reindex even if there was an error in one of the uploads, to make sure other changed tables are up to date.
    try:
        OpalFile.reindex_all()
    except Exception as e:
        handle_exception(e)
        retcode |= 2

    try:
        OpalFile.cleanup_binaries()
        logging.info('done')
    except Exception as e:
        handle_exception(e)
        retcode |= 4

    sys.exit(retcode)
    

if __name__ == '__main__':
    setup()
    main()
